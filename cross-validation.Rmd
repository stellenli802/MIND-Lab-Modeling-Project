---
title: "Cross-Validation-Metrics"
author: "Kyle Grosser"
date: "2025-06-04"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(survival)
library(pec)
library(rsample)
library(survAUC)
library(xtable)

source("G:/projects/tpgarcia/enroll-hd/02-Kyle-Comparing-Time-To-Diagnosis-Models/abby_git/MIND-Lab-Modeling-Project/metric_functions.R")
source("G:/projects/tpgarcia/enroll-hd/02-Kyle-Comparing-Time-To-Diagnosis-Models/abby_git/MIND-Lab-Modeling-Project/Langbehn_functions.R")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Data Preprocessing

```{r load-and-filter-data}
## Load analytic dataset
ENROLL <- readRDS("G:/projects/tpgarcia/enroll-hd/02-Kyle-Comparing-Time-To-Diagnosis-Models/enrollhd_baseline_survival_data.rds") %>%
  ungroup()

all_models_data_complete <- ENROLL %>%
  filter(W != 0) %>%
  filter(complete.cases(select(., W, delta, CAG, age_0,
                               diagconf, motscore, scnt1, swrt1,
                               sit1, sdmt1))) %>%
  filter(CAG > 40 & CAG < 57) %>%
  mutate(
    W_age = age_0 + W,
    motscore2 = motscore^2,
    CAP = age_0 * (CAG - 34),
    diagconf1 = ifelse(diagconf == 1, 1, 0),
    diagconf2 = ifelse(diagconf == 2, 1, 0),
    diagconf3 = ifelse(diagconf == 3, 1, 0)
  )

dim(all_models_data_complete)

1 - mean(all_models_data_complete$delta)
```

After pre-processing, our sample consisted of 5722 patients. We then filtered out patients with values of $W=0$ since these patients have no follow-up time. This first filtering step resulted in a sample of 4496 patients. We then filtered our dataset to have complete cases on the "union" of all covariates for each model. We filtered in this way so that our metrics were computed on the same sample for each model, and to keep the cross-validation procedure simple and interpretable. This complete case filtering procedure retains $4155$ patients with all variables required for the Langbehn, CAP, MRS, and PI models.

Lastly, we restricted our data to only include patients with CAG repeat lengths in the range $[41, 56]$ to match the work done in the Langbehn paper. This step reduced our sample size to $n = 3113$, which is a reduction of about $25%$. However, it is important to note that only $4$ of these patients were removed because their CAG repeat length was $>56$, while the remaining 1038 of those removed had a CAG repeat length $<41$. This particular range of CAG repeat lengths, $[36, 40]$, actually represents patients whose genotype is only partially penetrant; it could be argued that for a preventative clinical trial, it would be ideal to only recruit patients whose genotype is fully penetrant, since they are guaranteed to eventually reach the threshold for a clinical diagnosis whereas those in the partially penetrant range are not.

---

## 2. Split data with stratification

```{r create-CV-folds}
# set random seed
set.seed(2024)
n_splits = 5

# create training-testing folds, with stratification by delta to preserve censoring rate
folds <- vfold_cv(all_models_data_complete, v = n_splits, strata = delta)

# summarize censoring information for each training and testing split
fold_censoring_summary <- map_dfr(1:n_splits, function(i) {
  test_data <- assessment(folds$splits[[i]])
  event_rate <- mean(test_data$delta == 1)
  tibble(
    fold = i,
    event_rate = event_rate,
    censoring_rate = 1 - event_rate,
    n = nrow(test_data),
    n_events = sum(test_data$delta == 1)
  )
})
print(fold_censoring_summary)
```

```{r store-models}
# Preallocate output for models
fitted_models_by_fold <- vector("list", length = n_splits)

for (i in 1:n_splits) {
  # training-testing split i
  split <- folds$splits[[i]]
  
  # training set i
  train_data <- analysis(split)
  
  # testing set i
  test_data  <- assessment(split)

  # CAP model
  CAP_model <- survreg(Surv(W, delta) ~ age_0 + CAG:age_0,
                       data = train_data, dist = "loglogistic")

  # MRS model
  MRS_model <- coxph(Surv(W, delta) ~ factor(diagconf) +
                       motscore + motscore2 +
                       scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
                       CAG:motscore + CAG:age_0,
                     data = train_data, x = TRUE)

  # PI model
  PI_model <- coxph(Surv(W, delta) ~ motscore + sdmt1 + CAP,
                    data = train_data, x = TRUE)

  Langbehn_params <- tryCatch(
  fit_langbehn_model(train_data, decay_vals = c(0.13, 0.14)),
  error = function(e) {
    message(paste("Langbehn fit failed on fold", i, ":", e$message))
    return(NULL)
    }
  )

  # Linear predictors (negate for AFT to maintain directionality)
  lp_CAP <- -predict(CAP_model, newdata = test_data, type = "lp")
  lp_MRS <-  predict(MRS_model, newdata = test_data, type = "lp")
  lp_PI  <-  predict(PI_model,  newdata = test_data, type = "lp")
  
   # Langbehn-based risk score (CDF of diagnosis in next 5 years)
  risk_Langbehn <- Langbehn_cond_CDF(Langbehn_params,
                                     current_age = test_data$age_0,
                                     time_to_diag = 5,
                                     cag = test_data$CAG)
  
  # Store models, linear predictors, and training and testing data
  fitted_models_by_fold[[i]] <- list(
    CAP = CAP_model,
    MRS = MRS_model,
    PI = PI_model,
    Langbehn = Langbehn_params,
    lp_CAP = lp_CAP,
    lp_MRS = lp_MRS,
    lp_PI  = lp_PI,
    lp_Langbehn = risk_Langbehn,
    train_data = train_data,
    test_data = test_data
  )
}

# Extract Langbehn parameter estimates across folds
langbehn_params_all <- map(fitted_models_by_fold, ~ .x$Langbehn) %>%
  do.call(rbind, .) %>%
  as.data.frame()

# Optional: name the parameters for clarity
colnames(langbehn_params_all) <- c("mu_intercept", "mu_scale", "mu_decay",
                                   "sigma_intercept", "sigma_scale", "sigma_decay")

# Show the parameter estimates across folds
print(langbehn_params_all)
```

---

## 3. Harrell's C index

```{r}
# Preallocate output
c_index_list <- list()

for (i in 1:5) {
  # grab stored models and testing data
  models <- fitted_models_by_fold[[i]]
  test_data  <- models$test_data

  # save Harrell's C index estimates for fold i
  c_index_list[[i]] <- tibble(
    fold = i,
    model = c("CAP", "MRS", "PI"),
    c_index = c(
      getHarrellC(fit = models$CAP, data = test_data),
      getHarrellC(fit = models$MRS, data = test_data),
      getHarrellC(fit = models$PI,  data = test_data)
      # Optional: Compute C for Langbehn if convergence succeeded
      # Langbehn_Harrell_C <- getHarrellC(fit = Langbehn_params, data = test_data, model_type = "langbehn")
    )
  )
}

# Combine results
C_index_results <- bind_rows(c_index_list)

# Summarize Harrell's C index estimates across splits
C_index_summary <- C_index_results %>%
  group_by(model) %>%
  summarize(mean_c = mean(c_index), sd_c = sd(c_index), .groups = "drop")
print(C_index_summary)
```

Harrell's C index measures agreement between the ranking of patients by observed time to diagnosis and the ranking of patients by risk score (in reverse). Values close to 1 indicate strong agreement and values below 0.5 indicate poor performance. All of the mean Harrell's C for each model are below 0.2, indicating very poor performance. However, Harrell's C is known to be biased when censoring is high, which is the case in our sample (nearly 80% censoring).

---

## 4. Time-dependent, IPCW-based UNO's C / ROC curves

```{r IPCW-roc-curves}
CV_UnoC_results <- map_dfr(1:5, function(i) {
  # grab stored models and testing data
  models <- fitted_models_by_fold[[i]]
  train_data <- models$train_data
  test_data  <- models$test_data
  
  # time horizons at which Uno's C is calculated
  t_stars <- 1:3
  
  train_surv <- Surv(train_data$W, train_data$delta)
  test_surv  <- Surv(test_data$W, test_data$delta)

  # Time-specific Uno's C
  time_specific <- map_dfr(t_stars, function(t_star) {
    tibble(
      fold = i,
      time = t_star,
      Langbehn = AUC.uno(train_surv, test_surv, models$lp_Langbehn, times = t_star)$iauc,
      CAP = AUC.uno(train_surv, test_surv, models$lp_CAP, times = t_star)$iauc,
      MRS = AUC.uno(train_surv, test_surv, models$lp_MRS, times = t_star)$iauc,
      PI  = AUC.uno(train_surv, test_surv, models$lp_PI,  times = t_star)$iauc
    )
  })

   # Estimate weights = Pr(event at t_star) using test data
  weights <- map_dbl(t_stars, function(t) {
    sum(train_data$W == t & train_data$delta == 1)
  })
  weights <- weights / sum(weights)  # normalize to sum to 1

  # Compute weighted average Uno's C
  global <- tibble(
    fold = i,
    time = NA,
    Langbehn = weighted.mean(time_specific$Langbehn, weights, na.rm = TRUE),
    CAP = weighted.mean(time_specific$CAP, weights, na.rm = TRUE),
    MRS = weighted.mean(time_specific$MRS, weights, na.rm = TRUE),
    PI  = weighted.mean(time_specific$PI,  weights, na.rm = TRUE)
  )

  bind_rows(time_specific, global)
}) %>%
  pivot_longer(cols = c(Langbehn, CAP, MRS, PI), names_to = "model", values_to = "UnoC")
```

```{r Uno_C_table}
# Create summary table
UnoC_summary <- CV_UnoC_results %>%
  mutate(type = if_else(is.na(time), "Global", "Time-specific")) %>%
  group_by(model, type, time) %>%
  summarize(mean_UnoC = mean(UnoC), sd_UnoC = sd(UnoC), .groups = "drop") %>%
  arrange(type, time, model)

print(UnoC_summary)
```

```{r Uno_C_table_LaTeX}
# Format LaTeX table with rounded values
UnoC_summary_latex <- UnoC_summary %>%
  mutate(
    mean_UnoC = sprintf("%.3f", mean_UnoC),
    sd_UnoC   = sprintf("%.3f", sd_UnoC),
    UnoC_with_sd = paste0(mean_UnoC, " (", sd_UnoC, ")")
  ) %>%
  select(type, time, model, UnoC_with_sd) %>%
  pivot_wider(names_from = model, values_from = UnoC_with_sd)

# Generate xtable object
xtable_UnoC <- xtable(UnoC_summary_latex, 
                      caption = "Uno's C statistic (mean and SD across 5-fold CV) by model and evaluation time.",
                      label = "tab:UnoC_summary",
                      align = c("l", "l", "c", "c", "c", "c"))

print(xtable_UnoC, include.rownames = FALSE, sanitize.text.function = identity)
```


```{r Uno_C_plot}
# Compute global UnoC summaries for use in legend
global_labels <- CV_UnoC_results %>%
  filter(is.na(time)) %>%
  group_by(model) %>%
  summarize(mean_iAUC = mean(UnoC), .groups = "drop") %>%
  mutate(label = paste0(model, " (iAUC = ", round(mean_iAUC, 3), ")"))

# Create named vector for legend renaming
model_labels <- deframe(global_labels[, c("model", "label")])

# Plot only time-specific points (exclude global)
CV_UnoC_results %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = time, y = UnoC, color = model)) +
  geom_line(aes(group = interaction(model, fold)), alpha = 0.3) +
  geom_point(alpha = 0.6, size = 1.5) +
  stat_summary(fun = mean, geom = "line", aes(group = model), linewidth = 1.2) +
  scale_color_viridis_d(name = "Model (Global iAUC)", labels = model_labels) +
  labs(
    x = "Time (t*)",
    y = "Uno's C",
    title = "IPCW-AUC (Uno's C) Across 5-Fold CV",
    color = "Model"
  ) +
  theme_minimal(base_size = 13)
```

---

```{r}
# test_data: data for which Uno's C is to be calculated, containing 
#   W     = min(X, C), 
#   delta = I(X <= C)
#   lp    = linear predictor such that higher lp indicates higher risk
# G_hat: estimated survival function of C, used to calculate weights
# t_star: follow-up time of interest
compute_uno_c <- function(test_data, G_hat, t_star) {
  # Extract min times, event indicators, linear predictors, and sample size
  W  <- test_data$W
  d  <- test_data$delta
  lp <- test_data$lp
  n  <- length(W)

  # Create all pairwise combinations, dropping i == j
  pair_df <- expand.grid(i = 1:n, j = 1:n) %>%
    filter(i != j)

  # Extract values for each pair
  Ti <- W[pair_df$i]
  Tj <- W[pair_df$j]
  di <- d[pair_df$i]
  lpi <- lp[pair_df$i]
  lpj <- lp[pair_df$j]

  # Identify valid comparable pairs (Equation (6) in Uno et al. (2011))
  valid_pairs <- which(Ti < Tj & Ti <= t_star & di == 1)
  if (length(valid_pairs) == 0) return(NA_real_)

  # Calculate inverse probability of censoring weights
  G_Ti <- G_hat(Ti[valid_pairs])
  wi <- 1 / (G_Ti^2)

  # Concordance: a pair is concordant if the LP[i] > LP[j]
  concordant <- as.numeric(lpi[valid_pairs] > lpj[valid_pairs])

  # Compute Uno's C (Equation (6) in Uno et al. (2011))
  sum(wi * concordant) / sum(wi)
}

custom_uno_results <- map_dfr(1:5, function(fold_idx) {
  models <- fitted_models_by_fold[[fold_idx]]
  train_data <- models$train_data
  test_data  <- models$test_data

  # Fit KM for censoring distribution from training data
  km_cens <- survfit(Surv(W, 1 - delta) ~ 1, data = train_data)
  G_hat_left <- function(t) {
    s <- summary(km_cens, times = unique(c(0, km_cens$time)), extend = TRUE)
    approx(
      x = s$time,
      y = s$surv,
      xout = t - 1e-8,
      method = "constant",
      f = 0,
      rule = 2
    )$y
  }

  # Prepare output across models and t_star
  bind_rows(
    tibble(model = "Langbehn", lp = models$lp_Langbehn),
    tibble(model = "CAP", lp = models$lp_CAP),
    tibble(model = "MRS", lp = models$lp_MRS),
    tibble(model = "PIN", lp = models$lp_PI)
  ) %>%
    mutate(fold = fold_idx) %>%
    group_by(model, fold) %>%
    group_modify(~ {
      test_data$lp <- .x$lp
      map_dfr(1:8, function(t_star) {
        tibble(
          time = t_star,
          uno_c_custom = compute_uno_c(test_data, G_hat_left, t_star)
        )
      })
    })
})

print(custom_uno_results)
```

```{r}
# Time grid
t_grid <- 1:8

# Step 1: Gather per-fold, per-time event weights from training data
event_weights <- map_dfr(1:5, function(fold_idx) {
  train_data <- fitted_models_by_fold[[fold_idx]]$train_data

  # Event counts at each t* in training data
  weights <- map_dbl(t_grid, function(t) {
    sum(train_data$W == t & train_data$delta == 1)
  })
  weights <- weights / sum(weights)

  tibble(
    fold = fold_idx,
    time = t_grid,
    weight = weights
  )
})

# Step 2: Join weights with Uno’s C values and compute weighted mean per model/fold
global_iAUC <- custom_uno_results %>%
  filter(!is.na(time)) %>%
  left_join(event_weights, by = c("fold", "time")) %>%
  group_by(fold, model) %>%
  summarize(
    time = NA_integer_,
    uno_c_custom = weighted.mean(uno_c_custom, weight, na.rm = TRUE),
    .groups = "drop"
  )

# Step 3: Append to original custom_uno_results
custom_uno_results <- bind_rows(custom_uno_results, global_iAUC)
```

```{r}
# Chunk: UnoC_custom_table
UnoC_custom_summary <- custom_uno_results %>%
  group_by(model, time) %>%
  summarize(
    mean_UnoC = mean(uno_c_custom),
    sd_UnoC   = sd(uno_c_custom),
    .groups   = "drop"
  ) %>%
  mutate(type = "Custom") %>%
  select(type, time, model, mean_UnoC, sd_UnoC)

# Optional: sort NA time values to appear at top or bottom
UnoC_custom_summary <- UnoC_custom_summary %>%
  arrange(model, is.na(time), time)

# Format LaTeX output
UnoC_custom_summary_latex <- UnoC_custom_summary %>%
  mutate(
    time_label = ifelse(is.na(time), "Global", as.character(time)),
    mean_UnoC = sprintf("%.3f", mean_UnoC),
    sd_UnoC   = sprintf("%.3f", sd_UnoC),
    UnoC_with_sd = paste0(mean_UnoC, " (", sd_UnoC, ")")
  ) %>%
  select(time = time_label, model, UnoC_with_sd) %>%
  pivot_wider(names_from = model, values_from = UnoC_with_sd)

# Generate LaTeX table
xtable_UnoC_custom <- xtable(UnoC_custom_summary_latex,
                             caption = "Custom Uno's C statistic (mean and SD across 5-fold CV) by model and evaluation time (including global).",
                             label = "tab:UnoC_custom_summary",
                             align = c("l", "c", "c", "c", "c", "c"))

print(xtable_UnoC_custom, include.rownames = FALSE, sanitize.text.function = identity)
```

```{r custom-uno-c-plot}
# Compute global UnoC summaries for use in legend
global_labels_custom <- custom_uno_results %>%
  filter(is.na(time)) %>%
  group_by(model) %>%
  summarize(mean_iAUC = mean(uno_c_custom), .groups = "drop") %>%
  mutate(label = paste0(model, " (iAUC = ", round(mean_iAUC, 3), ")"))

# Create named vector for legend labels
model_labels_custom <- deframe(global_labels_custom[, c("model", "label")])

# Plot only time-specific values (exclude global NA)
custom_uno_results %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = time, y = uno_c_custom, color = model)) +
  geom_line(aes(group = interaction(model, fold)), alpha = 0.3) +
  geom_point(alpha = 0.6, size = 1.5) +
  stat_summary(fun = mean, geom = "line", aes(group = model), linewidth = 1.2) +
  scale_color_viridis_d(name = "Model (Global iAUC)", labels = model_labels_custom) +
  labs(
    x = "Time (t*)",
    y = "Uno's C",
    title = "IPCW-AUC (Uno's C) Across 5-Fold CV (Custom Calculation)",
    color = "Model"
  ) +
  theme_bw(base_size = 13)
```

```{r}
# compare custom Uno function to AUC.uno()
comparison_table <- custom_uno_results %>%
  mutate(model = case_when(model == "PIN" ~ "PI",
                           model != "PIN" ~ model)) %>%
  left_join(
    CV_UnoC_results %>% filter(!is.na(time)),
    by = c("fold", "model", "time")
  ) %>%
  rename(uno_c_aucuno = UnoC)

comparison_table %>%
  mutate(diff = uno_c_custom - uno_c_aucuno) %>%
  print(n = Inf) %>%
  view()
```

## 4.5 Published parameter IPCW-based UNO's C
```{r "pubparam setup and time-specific"}
# Langbehn Model Published Parameters
langbehn_params <- list(
  mu_intercept    =  21.54,
  mu_scale        =  9.56,
  mu_decay        =  0.146,
  sigma_intercept = 35.55,
  sigma_scale     = 17.72,
  sigma_decay     =  0.327
)
langbehn_params <- c(21.54, 9.56, 0.146, 35.55, 17.72, 0.327)

# MRS Model Published Parameters
mrs_params <- list(
  age_0       = -0.282,  # baseline age
  CAG         =  0.140,  # CAG repeat length
  motscore    =  0.565,  # Total Motor Score
  sdmt1       = -0.021,  # Symbol Digit Modalities Test
  diagconf1   =  0.347,  # Diagnostic Confidence Level 1
  diagconf2   =  0.542,  # Diagnostic Confidence Level 2
  diagconf3   =  1.086,  # Diagnostic Confidence Level 3
  scnt1       = -0.004,  # Stroop Color Naming
  swrt1       =  0.002,  # Stroop Word Reading
  sit1        = -0.023,  # Stroop Interference
  motscore2   = -0.004,  # TMS squared term
  motscore_CAG = -0.010, # TMS × CAG interaction
  age_CAG     =  0.009   # Age × CAG interaction
)

# PIN Model Published Parameters
pin_params <- list(
  motscore_coef =  51,    # Total Motor Score coefficient
  sdmt1_coef    = -34,    # SDMT coefficient
  CAP_coef      =   7,    # CAP score coefficient
  intercept     = -883,   # intercept
  scale         = 1044    # scale parameter
)

# CAP Model Published Parameters
cap_params <- list(
  intercept = 4.4196,
  age_0 = 0.2188,       # beta * C
  age_0_CAG = -0.0065   # beta
)

# CAP model linear predictor from published params
CAP_lp <- -with(all_models_data_complete, cap_params$intercept +
                         cap_params$age_0 * age_0 +
                         cap_params$age_0_CAG * (CAG * age_0))

# MRS model linear predictor from published params 
MRS_lp <- with(all_models_data_complete,
  mrs_params$diagconf1 * diagconf1 +
  mrs_params$diagconf2 * diagconf2 +
  mrs_params$diagconf3 * diagconf3 +
  mrs_params$motscore * motscore +
  mrs_params$motscore2 * motscore^2 +
  mrs_params$scnt1 * scnt1 +
  mrs_params$swrt1 * swrt1 +
  mrs_params$sit1 * sit1 +
  mrs_params$sdmt1 * sdmt1 +
  mrs_params$CAG * CAG +
  mrs_params$age_0 * age_0 +
  mrs_params$motscore_CAG * CAG * motscore +
  mrs_params$age_CAG * CAG * age_0
)

# PI model linear predictor from published params
PI_lp <- with(all_models_data_complete,
  pin_params$motscore_coef * motscore +
  pin_params$sdmt1_coef * sdmt1 +
  pin_params$CAP_coef * CAP
)
# Langbehn model risk calculation from published params
risk_Langbehn <- Langbehn_cond_CDF(langbehn_params,
                                   current_age = all_models_data_complete$W_age,
                                   time_to_diag = 5,
                                   cag = all_models_data_complete$CAG)

# Store published parameter-based model results
pubparam_models_full_data <- list(
  CAP_params = cap_params,
  MRS_params = mrs_params,
  PI_params  = pin_params,
  Langbehn_params = langbehn_params,
  lp_CAP = CAP_lp,
  lp_MRS = MRS_lp,
  lp_PI  = PI_lp,
  lp_Langbehn = risk_Langbehn
)

## Applying Kyle's custom uno's c function

# time horizons at which Uno's C is calculated
t_stars <- 1:8

# Fit KM for censoring distribution using full dataset
test_data_pubparam <- all_models_data_complete
km_cens <- survfit(Surv(W, 1 - delta) ~ 1, data = test_data_pubparam)

G_hat_left <- function(t) {
  s <- summary(km_cens, times = unique(c(0, km_cens$time)), extend = TRUE)
  approx(
    x = s$time,
    y = s$surv,
    xout = t - 1e-8,
    method = "constant",
    f = 0,
    rule = 2
  )$y
}

# Combine model lp vectors into a tidy format
all_models_lp <- bind_rows(
  tibble(model = "Langbehn", lp = risk_Langbehn),
  tibble(model = "CAP", lp = CAP_lp),
  tibble(model = "MRS", lp = MRS_lp),
  tibble(model = "PIN", lp = PI_lp)
)

# Calculate Uno's C for each model and t_star
custom_uno_results_pubparam <- all_models_lp %>%
  group_by(model) %>%
  group_modify(~ {
    test_data_pubparam$lp <- .x$lp  # assign lp vector to test_data

    map_dfr(t_stars, function(t_star) {
      tibble(
        time = t_star,
        uno_c_custom = compute_uno_c(test_data_pubparam, G_hat_left, t_star)
      )
    })
  })

print(custom_uno_results_pubparam)
```

```{r "pubparam global"}
# global AUC using custom Uno's C function
event_weights_pubparam <- tibble(
  time = t_stars,
  weight = map_dbl(t_stars, function(t) {
    sum(test_data_pubparam$W == t & test_data_pubparam$delta == 1)
  })
) %>%
  mutate(weight = weight / sum(weight))   # normalize to sum to 1


global_iAUC_pubparam <- custom_uno_results_pubparam %>%
  filter(!is.na(time)) %>%
  left_join(event_weights, by = c("time")) %>%
  group_by( model) %>%
  summarize(
    time = NA_integer_,
    uno_c_custom = weighted.mean(uno_c_custom, weight, na.rm = TRUE),
    .groups = "drop"
  )

# below now contains global and time-specific custom Uno's C for each of the four models with 
# their published parameters on the entire subsetted ENROLL-HD dataset
custom_uno_results_pubparam <- bind_rows(custom_uno_results_pubparam, global_iAUC_pubparam)
```

```{r "figures, tables, plots for pubparam"}
# figures, tables, and plots for published parameter-based Uno's C
UnoC_custom_summary_pubparam <- custom_uno_results_pubparam %>%
  group_by(model, time) %>%
  summarize(
    UnoC = mean(uno_c_custom),
    .groups   = "drop"
  ) %>%
  mutate(type = "Custom") %>%
  select(type, time, model, UnoC) %>%
  arrange(model, is.na(time), time)

# Format LaTeX output
UnoC_custom_summary_pubparam_latex <- UnoC_custom_summary_pubparam %>%
  mutate(
    time_label = ifelse(is.na(time), "Global", as.character(time)),
    UnoC = sprintf("%.3f", UnoC),
  ) %>%
  select(time = time_label, model, UnoC) %>%
  pivot_wider(names_from = model, values_from = UnoC)

# Generate LaTeX table
xtable_UnoC_custom_pubparam <- xtable(UnoC_custom_summary_pubparam_latex,
                             caption = "Custom Uno's C statistic by model and evaluation time (including global) for published model parameters.",
                             label = "tab:UnoC_custom_summary_pubparam",
                             align = c("l", "c", "c", "c", "c", "c"))

print(xtable_UnoC_custom_pubparam, include.rownames = FALSE, sanitize.text.function = identity)



## overlay published parameter plot with CV data-driven parameter plot from above

# Compute global UnoC summaries for use in legend
global_labels_custom <- custom_uno_results %>%
  filter(is.na(time)) %>%
  group_by(model) %>%
  summarize(mean_iAUC = mean(uno_c_custom), .groups = "drop") %>%
  mutate(label = paste0(model, " (iAUC = ", round(mean_iAUC, 3), ")"),
         params = "derived")
global_labels_custom_pubparam <- custom_uno_results_pubparam %>%
  filter(is.na(time)) %>%
  mutate(mean_iAUC = uno_c_custom,
         label = paste0(model, " (iAUC = ", round(uno_c_custom, 3), ")"),
         params = "published") %>%
  select(-time, -uno_c_custom)

global_labels_all <- rbind(global_labels_custom, global_labels_custom_pubparam)

# Prepare time-specific results with parameter type column
cv_results_time <- custom_uno_results %>%
  filter(!is.na(time)) %>%
  mutate(params = "derived")

pubparam_results_time <- custom_uno_results_pubparam %>%
  filter(!is.na(time)) %>%
  mutate(params = "published")

# Combine time-specific results for plotting
all_results_time <- bind_rows(cv_results_time, pubparam_results_time)

# Generate plot
combined_results <- bind_rows(
  cv_results_time %>%
    group_by(model, time) %>%
    summarize(uno_c_custom = mean(uno_c_custom), .groups = "drop") %>%
    mutate(params = "derived"),
  pubparam_results_time %>%
    mutate(params = "published")
)

ggplot(combined_results, aes(x = time, y = uno_c_custom, color = model, linetype = params, group = interaction(model, params))) +
  geom_line(linewidth = 1.2) +
  scale_color_viridis_d(name = "Model") +
  scale_linetype_manual(
    name = "Parameter Type",
    values = c(derived = "dashed", published = "solid")
  ) +
  labs(
    x = "Time (t*)",
    y = "Uno's C",
    title = "IPCW-AUC (Uno's C): Derived (mean) vs Published Parameters",
    color = "Model",
    linetype = "Parameters"
  ) +
  theme_bw(base_size = 13)
```


#### built-in uno c functions
```{r}
# Time-specific Uno's C
time_specific <- map_dfr(t_stars, function(t_star) {
  tibble(
    time = t_star,
    Langbehn = AUC.uno(full_data_surv, full_data_surv, risk_Langbehn, times = t_star)$iauc,
    CAP = AUC.uno(full_data_surv, full_data_surv, CAP_lp, times = t_star)$iauc,
    MRS = AUC.uno(full_data_surv, full_data_surv, MRS_lp, times = t_star)$iauc,
    PI  = AUC.uno(full_data_surv, full_data_surv, PI_lp,  times = t_star)$iauc
  )
})

 # Estimate weights = Pr(event at t_star) using test data
weights <- map_dbl(t_stars, function(t) {
  sum(all_models_data_complete$W == t & all_models_data_complete$delta == 1)
})
weights <- weights / sum(weights)  # normalize to sum to 1

# Compute weighted average Uno's C
global <- tibble(
  time = NA,
  Langbehn = weighted.mean(time_specific$Langbehn, weights, na.rm = TRUE),
  CAP = weighted.mean(time_specific$CAP, weights, na.rm = TRUE),
  MRS = weighted.mean(time_specific$MRS, weights, na.rm = TRUE),
  PI  = weighted.mean(time_specific$PI,  weights, na.rm = TRUE)
)

pubparam_UnoC_all_data <- bind_rows(time_specific, global) %>%
  pivot_longer(cols = c(Langbehn, CAP, MRS, PI), names_to = "model", values_to = "UnoC")

# Create summary table
pubparam_UnoC_all_data_summary <- pubparam_UnoC_all_data %>%
  mutate(type = if_else(is.na(time), "Global", "Time-specific")) %>%
  select(type, time, model, UnoC) %>%
  
  arrange(type, time, model)

print(pubparam_UnoC_all_data_summary)
```

#### old - Published parameter CV
```{r}
## apply models to ENROLL-HD with published parameters over each CV fold

pubparam_models_by_fold <- vector("list", length = n_splits)

for (i in 1:n_splits) {
  
  # training set i
  train_data <- fitted_models_by_fold[[i]]$train_data
  
  # testing set i
  test_data  <- fitted_models_by_fold[[i]]$test_data

  ## --- CAP model linear predictor from published params ---
  # Create design matrix matching the published model formula
  CAP_lp <- -with(test_data, cap_params$intercept +
                           cap_params$age_0 * age_0 +
                           cap_params$age_0_CAG * (CAG * age_0))
  
  ## --- MRS model linear predictor from published params ---
  MRS_lp <- with(test_data,
    mrs_params$diagconf1 * diagconf1 +
    mrs_params$diagconf2 * diagconf2 +
    mrs_params$diagconf3 * diagconf3 +
    mrs_params$motscore * motscore +
    mrs_params$motscore2 * motscore^2 +
    mrs_params$scnt1 * scnt1 +
    mrs_params$swrt1 * swrt1 +
    mrs_params$sit1 * sit1 +
    mrs_params$sdmt1 * sdmt1 +
    mrs_params$CAG * CAG +
    mrs_params$age_0 * age_0 +
    mrs_params$motscore_CAG * CAG * motscore +
    mrs_params$age_CAG * CAG * age_0
  )
  
  ## --- PI model linear predictor from published params ---
  PI_lp <- with(test_data,
    pin_params$motscore_coef * motscore +
    pin_params$sdmt1_coef * sdmt1 +
    pin_params$CAP_coef * CAP  # ensure CAP var is defined in test_data
  )
  ## --- Langbehn model risk calculation from published params ---
  risk_Langbehn <- Langbehn_cond_CDF(langbehn_params,
                                     current_age = test_data$W_age,
                                     time_to_diag = 5,
                                     cag = test_data$CAG)
  
  # Store published parameter-based model results
  pubparam_models_by_fold[[i]] <- list(
    CAP_params = cap_params,
    MRS_params = mrs_params,
    PI_params  = pin_params,
    Langbehn_params = langbehn_params,
    lp_CAP = CAP_lp,
    lp_MRS = MRS_lp,
    lp_PI  = PI_lp,
    lp_Langbehn = risk_Langbehn,
    train_data = train_data,
    test_data = test_data
  )
}

pubparam_CV_UnoC_results <- map_dfr(1:5, function(i) {
  # grab stored models and testing data
  models <- pubparam_models_by_fold[[i]]
  train_data <- models$train_data
  test_data  <- models$test_data
  
  # time horizons at which Uno's C is calculated
  t_stars <- 1:3
  
  train_surv <- Surv(train_data$W, train_data$delta)
  test_surv  <- Surv(test_data$W, test_data$delta)

  # Time-specific Uno's C
  time_specific <- map_dfr(t_stars, function(t_star) {
    tibble(
      fold = i,
      time = t_star,
      Langbehn = AUC.uno(train_surv, test_surv, models$lp_Langbehn, times = t_star)$iauc,
      CAP = AUC.uno(train_surv, test_surv, models$lp_CAP, times = t_star)$iauc,
      MRS = AUC.uno(train_surv, test_surv, models$lp_MRS, times = t_star)$iauc,
      PI  = AUC.uno(train_surv, test_surv, models$lp_PI,  times = t_star)$iauc
    )
  })

   # Estimate weights = Pr(event at t_star) using test data
  weights <- map_dbl(t_stars, function(t) {
    sum(train_data$W == t & train_data$delta == 1)
  })
  weights <- weights / sum(weights)  # normalize to sum to 1

  # Compute weighted average Uno's C
  global <- tibble(
    fold = i,
    time = NA,
    Langbehn = weighted.mean(time_specific$Langbehn, weights, na.rm = TRUE),
    CAP = weighted.mean(time_specific$CAP, weights, na.rm = TRUE),
    MRS = weighted.mean(time_specific$MRS, weights, na.rm = TRUE),
    PI  = weighted.mean(time_specific$PI,  weights, na.rm = TRUE)
  )

  bind_rows(time_specific, global)
}) %>%
  pivot_longer(cols = c(Langbehn, CAP, MRS, PI), names_to = "model", values_to = "UnoC")


# Create summary table
pubparam_UnoC_summary <- pubparam_CV_UnoC_results %>%
  mutate(type = if_else(is.na(time), "Global", "Time-specific")) %>%
  group_by(model, type, time) %>%
  summarize(mean_UnoC = mean(UnoC), sd_UnoC = sd(UnoC), .groups = "drop") %>%
  arrange(type, time, model)

print(pubparam_UnoC_summary)

```


## 5. Calibration Plots from 5-Fold CV

```{r calibration-data, include = F, cache = T}
CV_calibration_results <- map_dfr(1:5, function(i) {
  split <- folds$splits[[i]]
  train_data <- analysis(split)
  test_data  <- assessment(split)

  CAP_model <- survreg(Surv(W, delta) ~ age_0 + CAG:age_0,
                       data = train_data, dist = "loglogistic")
  MRS_model <- coxph(Surv(W, delta) ~ diagconf + motscore + motscore2 +
                       scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
                       CAG:motscore + CAG:age_0,
                     data = train_data, x = TRUE)
  PI_model <- coxph(Surv(W, delta) ~ motscore + sdmt1 + CAP,
                    data = train_data, x = TRUE)

  CAP_cal_data <- getCalibrationData(CAP_model, 1:5, test_data, "quantile", q=10, "CAP")
  MRS_cal_data <- getCalibrationData(MRS_model, 1:5, test_data, "quantile", q=10, "MRS")
  PI_cal_data  <- getCalibrationData(PI_model,  1:5, test_data, "quantile", q=10, "PI")

  CAP_cal_data$fold <- MRS_cal_data$fold <- PI_cal_data$fold <- i
  rbind(CAP_cal_data, MRS_cal_data, PI_cal_data)
})
```

```{r calibration-plots}
ggplot(CV_calibration_results, aes(x = Pred, y = Obs)) +
  geom_line(aes(group = fold), color = "black", alpha = 0.3) +
  geom_line(stat = "smooth", method = "loess", color = "blue", se = FALSE) +
  facet_grid(model ~ time, labeller = label_both) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "Predicted Risk", y = "Observed Proportion",
       title = "Pooled 5-Fold Calibration Curves") +
  theme_minimal()
```

In these plots, we facet by model (rows) and time horizon (columns). The first column ($t = 1$) shows how well each model predicts diagnosis within 1 year. A model demonstrates strong calibration if its results lie close to the line $y=x$ (the identity line). The grey lines indicate the performance of individual testing sets, while the blue lines indicates pooled performance. At $t=1$ and $t=2$, the MRS and PI models perform well. At $t=3$, $t=4$, and $t=5$, the CAP and MRS models perform well. Overall, the MRS model performs well across all horizons considered.

---

## 4. Time-Dependent ROC Curves

```{r roc-curves, cache = T}
CV_ROC_results <- map_dfr(1:5, function(i) {
  split <- folds$splits[[i]]
  train_data <- analysis(split)
  test_data  <- assessment(split)

  CAP_model <- survreg(Surv(W, delta) ~ age_0 + CAG:age_0,
                       data = train_data, dist = "loglogistic")
  MRS_model <- coxph(Surv(W, delta) ~ diagconf + motscore + motscore2 +
                       scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
                       CAG:motscore + CAG:age_0,
                     data = train_data, x = TRUE)
  PI_model <- coxph(Surv(W, delta) ~ motscore + sdmt1 + CAP,
                    data = train_data, x = TRUE)

  CAP_roc <- getSurvivalROCdata(CAP_model, 1:8, test_data, "W", "delta", "CAP")
  MRS_roc <- getSurvivalROCdata(MRS_model, 1:8, test_data, "W", "delta", "MRS")
  PI_roc  <- getSurvivalROCdata(PI_model,  1:8, test_data, "W", "delta", "PI")

  CAP_roc$fold <- MRS_roc$fold <- PI_roc$fold <- i
  bind_rows(CAP_roc, MRS_roc, PI_roc)
})
```

```{r roc-curves-results}
ggplot(CV_ROC_results, aes(x = FPR, y = TPR)) +
  geom_line(aes(group = fold), alpha = 0.3, color = "gray") +
  geom_line(stat = "smooth", method = "loess", se = FALSE, color = "blue") +
  facet_grid(model ~ time, labeller = label_both) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "False Positive Rate", y = "True Positive Rate",
       title = "Time-dependent ROC Curves: Pooled 5-Fold CV") +
  theme_minimal()

AUC_summary <- CV_ROC_results %>%
  group_by(model, time, fold) %>%
  summarize(AUC = unique(AUC)) %>%
  group_by(model, time) %>%
  summarize(mean_AUC = mean(AUC), sd_AUC = sd(AUC), .groups = "drop")

print(AUC_summary)
```

Time-dependent receiver operating characteristic (ROC) curves demonstrate a model's discriminative ability for individual time points. To summarise these curves, we calculate the area under the curve (AUC). AUC values near 1 indicate strong performance, while values close to 0 indicate poor performance. The CAP and PI models produce AUC values between 77 and 81, whereas the MRS model produces values between 85 and 87. Overall, these results indicate that the MRS model best distinguishes between patients who will be diagnosed by time $t$ and those who will not for all values of $t$ considered.

---

## 6. Brier score

```{r Brier-score}
times_eval <- 1:8

CV_brier_results <- map_dfr(1:5, function(i) {
  split <- folds$splits[[i]]
  train_data <- analysis(split)
  test_data <- assessment(split)

  # CAP_formula <- Surv(W, delta) ~ age_0 + CAG:age_0
  MRS_formula <- Surv(W, delta) ~ diagconf + motscore + motscore2 +
    scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
    CAG:motscore + CAG:age_0
  PI_formula <- Surv(W, delta) ~ motscore + sdmt1 + CAP

  # CAP_model <- survreg(CAP_formula, data = train_data, dist = "loglogistic")
  MRS_model <- coxph(MRS_formula, data = train_data, x = TRUE)
  PI_model  <- coxph(PI_formula, data = train_data, x = TRUE)

  # CAP_brier <- extractBrierSummary(getBrierScore(CAP_model, test_data, CAP_formula, times_eval))$brier_scores %>%
    # mutate(model = "CAP", fold = i)

  MRS_brier <- extractBrierSummary(getBrierScore(MRS_model, test_data, MRS_formula, times_eval))$brier_scores %>%
    mutate(model = "MRS", fold = i)

  PI_brier <- extractBrierSummary(getBrierScore(PI_model, test_data, PI_formula, times_eval))$brier_scores %>%
    mutate(model = "PI", fold = i)

  bind_rows(# CAP_brier,
            MRS_brier, PI_brier)
})

brier_summary_grouped <- CV_brier_results %>%
  group_by(model, time) %>%
  summarize(mean_brier = mean(brier),
            sd_brier = sd(brier),
            .groups = "drop")

print(brier_summary_grouped)
```