---
title: "Cross-Validation-Metrics"
author: "Kyle Grosser"
date: "2025-06-04"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(survival)
library(pec)
library(rsample)
library(survAUC)
library(xtable)

source("G:/projects/tpgarcia/enroll-hd/02-Kyle-Comparing-Time-To-Diagnosis-Models/abby_git/MIND-Lab-Modeling-Project/metric_functions.R")
source("G:/projects/tpgarcia/enroll-hd/02-Kyle-Comparing-Time-To-Diagnosis-Models/abby_git/MIND-Lab-Modeling-Project/Langbehn_functions.R")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Data Preprocessing

```{r load-and-filter-data}
## Load analytic dataset
ENROLL <- readRDS("G:/projects/tpgarcia/enroll-hd/02-Kyle-Comparing-Time-To-Diagnosis-Models/enrollhd_baseline_survival_data.rds") %>%
  ungroup()

all_models_data_complete <- ENROLL %>%
  filter(W != 0) %>%
  filter(complete.cases(select(., W, delta, CAG, age_0,
                               diagconf, motscore, scnt1, swrt1,
                               sit1, sdmt1))) %>%
  filter(CAG > 40 & CAG < 57) %>%
  mutate(
    W_age = age_0 + W,
    motscore2 = motscore^2,
    CAP = age_0 * (CAG - 34)
  )

dim(all_models_data_complete)

1 - mean(all_models_data_complete$delta)
```

After pre-processing, our sample consisted of 5722 patients. We then filtered out patients with values of $W=0$ since these patients have no follow-up time. This first filtering step resulted in a sample of 4496 patients. We then filtered our dataset to have complete cases on the "union" of all covariates for each model. We filtered in this way so that our metrics were computed on the same sample for each model, and to keep the cross-validation procedure simple and interpretable. This complete case filtering procedure retains $4155$ patients with all variables required for the Langbehn, CAP, MRS, and PI models.

Lastly, we restricted our data to only include patients with CAG repeat lengths in the range $[41, 56]$ to match the work done in the Langbehn paper. This step reduced our sample size to $n = 3113$, which is a reduction of about $25%$. However, it is important to note that only $4$ of these patients were removed because their CAG repeat length was $>56$, while the remaining 1038 of those removed had a CAG repeat length $<41$. This particular range of CAG repeat lengths, $[36, 40]$, actually represents patients whose genotype is only partially penetrant; it could be argued that for a preventative clinical trial, it would be ideal to only recruit patients whose genotype is fully penetrant, since they are guaranteed to eventually reach the threshold for a clinical diagnosis whereas those in the partially penetrant range are not.

---

## 2. Split data with stratification

```{r create-CV-folds}
# set random seed
set.seed(2024)

# create training-testing folds, with stratification by delta to preserve censoring rate
folds <- vfold_cv(all_models_data_complete, v = 5, strata = delta)

# summarize censoring information for each training and testing split
fold_censoring_summary <- map_dfr(1:5, function(i) {
  test_data <- assessment(folds$splits[[i]])
  event_rate <- mean(test_data$delta == 1)
  tibble(
    fold = i,
    event_rate = event_rate,
    censoring_rate = 1 - event_rate,
    n = nrow(test_data),
    n_events = sum(test_data$delta == 1)
  )
})
print(fold_censoring_summary)
```

```{r store-models}
# Preallocate output for models
fitted_models_by_fold <- vector("list", length = 5)

for (i in 1:5) {
  # training-testing split i
  split <- folds$splits[[i]]
  
  # training set i
  train_data <- analysis(split)
  
  # testing set i
  test_data  <- assessment(split)

  # CAP model
  CAP_model <- survreg(Surv(W, delta) ~ age_0 + CAG:age_0,
                       data = train_data, dist = "loglogistic")

  # MRS model
  MRS_model <- coxph(Surv(W, delta) ~ diagconf + motscore + motscore2 +
                       scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
                       CAG:motscore + CAG:age_0,
                     data = train_data, x = TRUE)

  # PI model
  PI_model <- coxph(Surv(W, delta) ~ motscore + sdmt1 + CAP,
                    data = train_data, x = TRUE)

  # Langbehn_params <- fit_langbehn_model(train_data, verbose = FALSE)

  # Linear predictors (negate for AFT to maintain directionality)
  lp_CAP <- -predict(CAP_model, newdata = test_data, type = "lp")
  lp_MRS <-  predict(MRS_model, newdata = test_data, type = "lp")
  lp_PI  <-  predict(PI_model,  newdata = test_data, type = "lp")
  
  # Store models, linear predictors, and training and testing data
  fitted_models_by_fold[[i]] <- list(
    CAP = CAP_model,
    MRS = MRS_model,
    PI = PI_model,
    # Langbehn = Langbehn_params,
    lp_CAP = lp_CAP,
    lp_MRS = lp_MRS,
    lp_PI  = lp_PI,
    train_data = train_data,
    test_data = test_data
  )
}
```

---

## 3. Harrell's C index

```{r}
# Preallocate output
c_index_list <- list()

for (i in 1:5) {
  # grab stored models and testing data
  models <- fitted_models_by_fold[[i]]
  test_data  <- models$test_data

  # save Harrell's C index estimates for fold i
  c_index_list[[i]] <- tibble(
    fold = i,
    model = c("CAP", "MRS", "PI"),
    c_index = c(
      getHarrellC(fit = models$CAP, data = test_data),
      getHarrellC(fit = models$MRS, data = test_data),
      getHarrellC(fit = models$PI,  data = test_data)
      # Optional: Compute C for Langbehn if convergence succeeded
      # Langbehn_Harrell_C <- getHarrellC(fit = Langbehn_params, data = test_data, model_type = "langbehn")
    )
  )
}

# Combine results
C_index_results <- bind_rows(c_index_list)

# Summarize Harrell's C index estimates across splits
C_index_summary <- C_index_results %>%
  group_by(model) %>%
  summarize(mean_c = mean(c_index), sd_c = sd(c_index), .groups = "drop")
print(C_index_summary)
```

Harrell's C index measures agreement between the ranking of patients by observed time to diagnosis and the ranking of patients by risk score (in reverse). Values close to 1 indicate strong agreement and values below 0.5 indicate poor performance. All of the mean Harrell's C for each model are below 0.2, indicating very poor performance. However, Harrell's C is known to be biased when censoring is high, which is the case in our sample (nearly 80% censoring).

---

## 4. Time-dependent, IPCW-based ROC curves

```{r IPCW-roc-curves}
CV_UnoC_results <- map_dfr(1:5, function(i) {
  # grab stored models and testing data
  models <- fitted_models_by_fold[[i]]
  train_data <- models$train_data
  test_data  <- models$test_data
  
  # time horizons at which Uno's C is calculated
  t_stars <- 1:3
  
  train_surv <- Surv(train_data$W, train_data$delta)
  test_surv  <- Surv(test_data$W, test_data$delta)

  # Time-specific Uno's C
  time_specific <- map_dfr(t_stars, function(t_star) {
    tibble(
      fold = i,
      time = t_star,
      CAP = AUC.uno(train_surv, test_surv, models$lp_CAP, times = t_star)$iauc,
      MRS = AUC.uno(train_surv, test_surv, models$lp_MRS, times = t_star)$iauc,
      PI  = AUC.uno(train_surv, test_surv, models$lp_PI,  times = t_star)$iauc
    )
  })

   # Estimate weights = Pr(event at t_star) using test data
  weights <- map_dbl(t_stars, function(t) {
    sum(train_data$W == t & train_data$delta == 1)
  })
  weights <- weights / sum(weights)  # normalize to sum to 1

  # Compute weighted average Uno's C
  global <- tibble(
    fold = i,
    time = NA,
    CAP = weighted.mean(time_specific$CAP, weights, na.rm = TRUE),
    MRS = weighted.mean(time_specific$MRS, weights, na.rm = TRUE),
    PI  = weighted.mean(time_specific$PI,  weights, na.rm = TRUE)
  )

  bind_rows(time_specific, global)
}) %>%
  pivot_longer(cols = c(CAP, MRS, PI), names_to = "model", values_to = "UnoC")
```

```{r Uno_C_table}
# Create summary table
UnoC_summary <- CV_UnoC_results %>%
  mutate(type = if_else(is.na(time), "Global", "Time-specific")) %>%
  group_by(model, type, time) %>%
  summarize(mean_UnoC = mean(UnoC), sd_UnoC = sd(UnoC), .groups = "drop") %>%
  arrange(type, time, model)

print(UnoC_summary)
```

```{r Uno_C_table_LaTeX}
# Format LaTeX table with rounded values
UnoC_summary_latex <- UnoC_summary %>%
  mutate(
    mean_UnoC = sprintf("%.3f", mean_UnoC),
    sd_UnoC   = sprintf("%.3f", sd_UnoC),
    UnoC_with_sd = paste0(mean_UnoC, " (", sd_UnoC, ")")
  ) %>%
  select(type, time, model, UnoC_with_sd) %>%
  pivot_wider(names_from = model, values_from = UnoC_with_sd)

# Generate xtable object
xtable_UnoC <- xtable(UnoC_summary_latex, 
                      caption = "Uno's C statistic (mean and SD across 5-fold CV) by model and evaluation time.",
                      label = "tab:UnoC_summary",
                      align = c("l", "l", "c", "c", "c", "c"))

print(xtable_UnoC, include.rownames = FALSE, sanitize.text.function = identity)
```


```{r Uno_C_plot}
# Compute global UnoC summaries for use in legend
global_labels <- CV_UnoC_results %>%
  filter(is.na(time)) %>%
  group_by(model) %>%
  summarize(mean_iAUC = mean(UnoC), .groups = "drop") %>%
  mutate(label = paste0(model, " (iAUC = ", round(mean_iAUC, 3), ")"))

# Create named vector for legend renaming
model_labels <- deframe(global_labels[, c("model", "label")])

# Plot only time-specific points (exclude global)
CV_UnoC_results %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = time, y = UnoC, color = model)) +
  geom_line(aes(group = interaction(model, fold)), alpha = 0.3) +
  geom_point(alpha = 0.6, size = 1.5) +
  stat_summary(fun = mean, geom = "line", aes(group = model), linewidth = 1.2) +
  scale_color_viridis_d(name = "Model (Global iAUC)", labels = model_labels) +
  labs(
    x = "Time (t*)",
    y = "Uno's C",
    title = "IPCW-AUC (Uno's C) Across 5-Fold CV",
    color = "Model"
  ) +
  theme_minimal(base_size = 13)
```

---

```{r}
compute_uno_c <- function(test_data, G_hat, t_star) {
  # Extract min times, event indicators, linear predictors, and sample size
  W  <- test_data$W
  d  <- test_data$delta
  lp <- test_data$lp
  n  <- length(W)

  # Create all pairwise combinations
  pair_df <- expand.grid(i = 1:n, j = 1:n) %>%
    filter(i != j)

  # Extract values for each pair
  Ti <- W[pair_df$i]
  Tj <- W[pair_df$j]
  di <- d[pair_df$i]
  lpi <- lp[pair_df$i]
  lpj <- lp[pair_df$j]

  # Identify valid comparable pairs
  valid_pairs <- which(Ti < Tj & Ti <= t_star & di == 1)
  if (length(valid_pairs) == 0) return(NA_real_)

  # Weights
  G_Ti <- G_hat(Ti[valid_pairs])
  wi <- 1 / (G_Ti^2)

  # Concordance
  concordant <- as.numeric(lpi[valid_pairs] > lpj[valid_pairs])

  # Compute Uno's C
  sum(wi * concordant) / sum(wi)
}

custom_uno_results <- map_dfr(1:5, function(fold_idx) {
  models <- fitted_models_by_fold[[fold_idx]]
  train_data <- models$train_data
  test_data  <- models$test_data

  # Fit KM for censoring distribution from training data
  km_cens <- survfit(Surv(W, 1 - delta) ~ 1, data = train_data)
  G_hat_left <- function(t) {
    s <- summary(km_cens, times = unique(c(0, km_cens$time)), extend = TRUE)
    approx(
      x = s$time,
      y = s$surv,
      xout = t - 1e-8,
      method = "constant",
      f = 0,
      rule = 2
    )$y
  }

  # Prepare output across models and t_star
  bind_rows(
    tibble(model = "CAP", lp = models$lp_CAP),
    tibble(model = "MRS", lp = models$lp_MRS),
    tibble(model = "PIN", lp = models$lp_PI)
  ) %>%
    mutate(fold = fold_idx) %>%
    group_by(model, fold) %>%
    group_modify(~ {
      test_data$lp <- .x$lp
      map_dfr(1:8, function(t_star) {
        tibble(
          time = t_star,
          uno_c_custom = compute_uno_c(test_data, G_hat_left, t_star)
        )
      })
    })
})

print(custom_uno_results)
```

```{r}
# Time grid
t_grid <- 1:8

# Step 1: Gather per-fold, per-time event weights from training data
event_weights <- map_dfr(1:5, function(fold_idx) {
  train_data <- fitted_models_by_fold[[fold_idx]]$train_data

  # Event counts at each t* in training data
  weights <- map_dbl(t_grid, function(t) {
    sum(train_data$W == t & train_data$delta == 1)
  })
  weights <- weights / sum(weights)

  tibble(
    fold = fold_idx,
    time = t_grid,
    weight = weights
  )
})

# Step 2: Join weights with Unoâ€™s C values and compute weighted mean per model/fold
global_iAUC <- custom_uno_results %>%
  filter(!is.na(time)) %>%
  left_join(event_weights, by = c("fold", "time")) %>%
  group_by(fold, model) %>%
  summarize(
    time = NA_integer_,
    uno_c_custom = weighted.mean(uno_c_custom, weight, na.rm = TRUE),
    .groups = "drop"
  )

# Step 3: Append to original custom_uno_results
custom_uno_results <- bind_rows(custom_uno_results, global_iAUC)
```

```{r}
# Chunk: UnoC_custom_table
UnoC_custom_summary <- custom_uno_results %>%
  group_by(model, time) %>%
  summarize(
    mean_UnoC = mean(uno_c_custom),
    sd_UnoC   = sd(uno_c_custom),
    .groups   = "drop"
  ) %>%
  mutate(type = "Custom") %>%
  select(type, time, model, mean_UnoC, sd_UnoC)

# Optional: sort NA time values to appear at top or bottom
UnoC_custom_summary <- UnoC_custom_summary %>%
  arrange(model, is.na(time), time)

# Format LaTeX output
UnoC_custom_summary_latex <- UnoC_custom_summary %>%
  mutate(
    time_label = ifelse(is.na(time), "Global", as.character(time)),
    mean_UnoC = sprintf("%.3f", mean_UnoC),
    sd_UnoC   = sprintf("%.3f", sd_UnoC),
    UnoC_with_sd = paste0(mean_UnoC, " (", sd_UnoC, ")")
  ) %>%
  select(time = time_label, model, UnoC_with_sd) %>%
  pivot_wider(names_from = model, values_from = UnoC_with_sd)

# Generate LaTeX table
xtable_UnoC_custom <- xtable(UnoC_custom_summary_latex,
                             caption = "Custom Uno's C statistic (mean and SD across 5-fold CV) by model and evaluation time (including global).",
                             label = "tab:UnoC_custom_summary",
                             align = c("l", "c", "c", "c", "c"))

print(xtable_UnoC_custom, include.rownames = FALSE, sanitize.text.function = identity)
```

```{r custom-uno-c-plot}
# Compute global UnoC summaries for use in legend
global_labels_custom <- custom_uno_results %>%
  filter(is.na(time)) %>%
  group_by(model) %>%
  summarize(mean_iAUC = mean(uno_c_custom), .groups = "drop") %>%
  mutate(label = paste0(model, " (iAUC = ", round(mean_iAUC, 3), ")"))

# Create named vector for legend labels
model_labels_custom <- deframe(global_labels_custom[, c("model", "label")])

# Plot only time-specific values (exclude global NA)
custom_uno_results %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = time, y = uno_c_custom, color = model)) +
  geom_line(aes(group = interaction(model, fold)), alpha = 0.3) +
  geom_point(alpha = 0.6, size = 1.5) +
  stat_summary(fun = mean, geom = "line", aes(group = model), linewidth = 1.2) +
  scale_color_viridis_d(name = "Model (Global iAUC)", labels = model_labels_custom) +
  labs(
    x = "Time (t*)",
    y = "Uno's C",
    title = "IPCW-AUC (Uno's C) Across 5-Fold CV (Custom Calculation)",
    color = "Model"
  ) +
  theme_bw(base_size = 13)
```

```{r}
# compare custom Uno function to AUC.uno()
comparison_table <- custom_uno_results %>%
  mutate(model = case_when(model == "PIN" ~ "PI",
                           model != "PIN" ~ model)) %>%
  left_join(
    CV_UnoC_results %>% filter(!is.na(time)),
    by = c("fold", "model", "time")
  ) %>%
  rename(uno_c_aucuno = UnoC)

comparison_table %>%
  mutate(diff = uno_c_custom - uno_c_aucuno) %>%
  print(n = Inf) %>%
  view()
```

## 5. Calibration Plots from 5-Fold CV

```{r calibration-data, include = F, cache = T}
CV_calibration_results <- map_dfr(1:5, function(i) {
  split <- folds$splits[[i]]
  train_data <- analysis(split)
  test_data  <- assessment(split)

  CAP_model <- survreg(Surv(W, delta) ~ age_0 + CAG:age_0,
                       data = train_data, dist = "loglogistic")
  MRS_model <- coxph(Surv(W, delta) ~ diagconf + motscore + motscore2 +
                       scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
                       CAG:motscore + CAG:age_0,
                     data = train_data, x = TRUE)
  PI_model <- coxph(Surv(W, delta) ~ motscore + sdmt1 + CAP,
                    data = train_data, x = TRUE)

  CAP_cal_data <- getCalibrationData(CAP_model, 1:5, test_data, "quantile", q=10, "CAP")
  MRS_cal_data <- getCalibrationData(MRS_model, 1:5, test_data, "quantile", q=10, "MRS")
  PI_cal_data  <- getCalibrationData(PI_model,  1:5, test_data, "quantile", q=10, "PI")

  CAP_cal_data$fold <- MRS_cal_data$fold <- PI_cal_data$fold <- i
  rbind(CAP_cal_data, MRS_cal_data, PI_cal_data)
})
```

```{r calibration-plots}
ggplot(CV_calibration_results, aes(x = Pred, y = Obs)) +
  geom_line(aes(group = fold), color = "black", alpha = 0.3) +
  geom_line(stat = "smooth", method = "loess", color = "blue", se = FALSE) +
  facet_grid(model ~ time, labeller = label_both) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "Predicted Risk", y = "Observed Proportion",
       title = "Pooled 5-Fold Calibration Curves") +
  theme_minimal()
```

In these plots, we facet by model (rows) and time horizon (columns). The first column ($t = 1$) shows how well each model predicts diagnosis within 1 year. A model demonstrates strong calibration if its results lie close to the line $y=x$ (the identity line). The grey lines indicate the performance of individual testing sets, while the blue lines indicates pooled performance. At $t=1$ and $t=2$, the MRS and PI models perform well. At $t=3$, $t=4$, and $t=5$, the CAP and MRS models perform well. Overall, the MRS model performs well across all horizons considered.

---

## 4. Time-Dependent ROC Curves

```{r roc-curves, cache = T}
CV_ROC_results <- map_dfr(1:5, function(i) {
  split <- folds$splits[[i]]
  train_data <- analysis(split)
  test_data  <- assessment(split)

  CAP_model <- survreg(Surv(W, delta) ~ age_0 + CAG:age_0,
                       data = train_data, dist = "loglogistic")
  MRS_model <- coxph(Surv(W, delta) ~ diagconf + motscore + motscore2 +
                       scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
                       CAG:motscore + CAG:age_0,
                     data = train_data, x = TRUE)
  PI_model <- coxph(Surv(W, delta) ~ motscore + sdmt1 + CAP,
                    data = train_data, x = TRUE)

  CAP_roc <- getSurvivalROCdata(CAP_model, 1:8, test_data, "W", "delta", "CAP")
  MRS_roc <- getSurvivalROCdata(MRS_model, 1:8, test_data, "W", "delta", "MRS")
  PI_roc  <- getSurvivalROCdata(PI_model,  1:8, test_data, "W", "delta", "PI")

  CAP_roc$fold <- MRS_roc$fold <- PI_roc$fold <- i
  bind_rows(CAP_roc, MRS_roc, PI_roc)
})
```

```{r roc-curves-results}
ggplot(CV_ROC_results, aes(x = FPR, y = TPR)) +
  geom_line(aes(group = fold), alpha = 0.3, color = "gray") +
  geom_line(stat = "smooth", method = "loess", se = FALSE, color = "blue") +
  facet_grid(model ~ time, labeller = label_both) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "False Positive Rate", y = "True Positive Rate",
       title = "Time-dependent ROC Curves: Pooled 5-Fold CV") +
  theme_minimal()

AUC_summary <- CV_ROC_results %>%
  group_by(model, time, fold) %>%
  summarize(AUC = unique(AUC)) %>%
  group_by(model, time) %>%
  summarize(mean_AUC = mean(AUC), sd_AUC = sd(AUC), .groups = "drop")

print(AUC_summary)
```

Time-dependent receiver operating characteristic (ROC) curves demonstrate a model's discriminative ability for individual time points. To summarise these curves, we calculate the area under the curve (AUC). AUC values near 1 indicate strong performance, while values close to 0 indicate poor performance. The CAP and PI models produce AUC values between 77 and 81, whereas the MRS model produces values between 85 and 87. Overall, these results indicate that the MRS model best distinguishes between patients who will be diagnosed by time $t$ and those who will not for all values of $t$ considered.

---

## 6. Brier score

```{r Brier-score}
times_eval <- 1:8

CV_brier_results <- map_dfr(1:5, function(i) {
  split <- folds$splits[[i]]
  train_data <- analysis(split)
  test_data <- assessment(split)

  # CAP_formula <- Surv(W, delta) ~ age_0 + CAG:age_0
  MRS_formula <- Surv(W, delta) ~ diagconf + motscore + motscore2 +
    scnt1 + swrt1 + sit1 + sdmt1 + CAG + age_0 +
    CAG:motscore + CAG:age_0
  PI_formula <- Surv(W, delta) ~ motscore + sdmt1 + CAP

  # CAP_model <- survreg(CAP_formula, data = train_data, dist = "loglogistic")
  MRS_model <- coxph(MRS_formula, data = train_data, x = TRUE)
  PI_model  <- coxph(PI_formula, data = train_data, x = TRUE)

  # CAP_brier <- extractBrierSummary(getBrierScore(CAP_model, test_data, CAP_formula, times_eval))$brier_scores %>%
    # mutate(model = "CAP", fold = i)

  MRS_brier <- extractBrierSummary(getBrierScore(MRS_model, test_data, MRS_formula, times_eval))$brier_scores %>%
    mutate(model = "MRS", fold = i)

  PI_brier <- extractBrierSummary(getBrierScore(PI_model, test_data, PI_formula, times_eval))$brier_scores %>%
    mutate(model = "PI", fold = i)

  bind_rows(# CAP_brier,
            MRS_brier, PI_brier)
})

brier_summary_grouped <- CV_brier_results %>%
  group_by(model, time) %>%
  summarize(mean_brier = mean(brier),
            sd_brier = sd(brier),
            .groups = "drop")

print(brier_summary_grouped)
```